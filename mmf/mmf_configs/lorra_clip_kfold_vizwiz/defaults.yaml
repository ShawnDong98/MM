model_config:
  lorra:
    image_feature_encodings:
    - type: default
      params: {}
    image_feature_dim: 2560 

dataset_config:
  vqa2:
    use_images: false
    use_features: true
    features:
      train:
      - /home/datasets/VizWiz/grid-feats-vqa/clip/pythia/RN50x4
      val:
      - /home/datasets/VizWiz/grid-feats-vqa/clip/pythia/RN50x4
      test:
      - /home/datasets/VizWiz/grid-feats-vqa/clip/pythia/RN50x4
    annotations:
      train:
      - ./datasets/vizwiz/pythia_annotations_paddleocr/imdb_vizwiz_train_fold0.npy
      val:
      - ./datasets/vizwiz/pythia_annotations_paddleocr/imdb_vizwiz_val_fold0.npy
      test:
      - ./datasets/vizwiz/pythia_annotations_paddleocr/imdb_vizwiz_test.npy
    use_ocr: True

optimizer:
  type: Adamax
  params:
    eps: 1.0e-08
    lr: 0.005
    weight_decay: 0

evaluation:
  metrics:
  - vqa_accuracy

training:
  seed: 42
  clip_norm_mode: all
  clip_gradients: true
  lr_ratio: 0.1
  lr_scheduler: true
  lr_steps:
  - 1250
  - 1500
  - 1750
  - 1825
  max_grad_l2_norm: 0.25
  max_updates: 2000
  use_warmup: true
  warmup_factor: 0.2
  warmup_iterations: 400
  batch_size: 256
  num_workers: 2
  task_size_proportional_sampling: true
  early_stop:
    criteria: vqa2/vqa_accuracy
    minimize: false

checkpoint:
  pretrained_state_mapping:
    word_embedding: word_embedding
    text_embeddings: text_embeddings
    image_feature_encoders: image_feature_encoders
    image_feature_embeddings_list: image_feature_embeddings_list
    image_text_multi_modal_combine_layer: image_text_multi_modal_combine_layer